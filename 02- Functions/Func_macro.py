
def imputer_valeurs(df):
    df_impute = df.copy()
    
    # ‚úÖ Identifier automatiquement les colonnes num√©riques (ann√©es)
    colonnes_annees = df.select_dtypes(include=[np.number]).columns.tolist()
    
    # ‚úÖ Calcul des valeurs globales d‚Äôimputation
    max_inflation = df.loc[df["Indicateur"] == "Inflation", colonnes_annees].max().max()
    max_chomage   = df.loc[df["Indicateur"] == "Chomage", colonnes_annees].max().max()
    min_gdp       = df.loc[df["Indicateur"] == "GDP", colonnes_annees].min().min()
    
    # ‚úÖ Application de l‚Äôimputation selon l‚Äôindicateur
    for indicateur, group in df.groupby("Indicateur"):
        if indicateur == "Inflation":
            group[colonnes_annees] = group[colonnes_annees].fillna(max_inflation)
        elif indicateur == "Chomage":
            group[colonnes_annees] = group[colonnes_annees].fillna(max_chomage)
        elif indicateur == "GDP":
            group[colonnes_annees] = group[colonnes_annees].fillna(min_gdp)
        
        # R√©int√©gration dans le DataFrame principal
        df_impute.loc[group.index, colonnes_annees] = group[colonnes_annees]
    
    return df_impute


# ============================
# ‚öôÔ∏è Param√®tres modifiables
# ============================

@dataclass
class MetricResult:
    country: str
    group: str
    n_obs: int
    risk_gdp: Optional[float]
    risk_infl: Optional[float]
    risk_unemp: Optional[float]


TARGET_INFL_LOWER = 2.0   # borne basse zone de confort inflation
TARGET_INFL_UPPER = 5.0   # borne haute zone de confort inflation
MIN_YEARS_REQUIRED = 1    # nb d'ann√©es min pour calculer des m√©triques fiables
WEIGHTS = {               # pond√©rations du score composite (somme = 1)
    "inflation": 0.40,
    "gdp": 0.35,
    "unemp": 0.25,
}
PERIOD = list(range(2010, 2025))


def _yoy_growth(series: pd.Series) -> pd.Series:
    # series.pct_change(periods=1) * 100.0
    return series

def _semivariance_negative(values: pd.Series) -> float:
    neg = values[values < 0]
    if len(neg) == 0:
        return 0.0
    return float(((neg - 0) ** 2).mean())


def _iqr_winsorize(s: pd.Series, k: float = 3.0) -> pd.Series:
    q1, q3 = s.quantile(0.25), s.quantile(0.75)
    iqr = q3 - q1
    lo, hi = q1 - k * iqr, q3 + k * iqr
    return s.clip(lower=lo, upper=hi)

# ========== Calcul rolling par pays √ó ann√©e ==========


def compute_country_metrics(df_long: pd.DataFrame) -> pd.DataFrame:
    # Filtre p√©riode
    d = df_long[df_long["Ann√©e"].isin(PERIOD)].copy()

    results: List[MetricResult] = []

    for (grp, ctry), sub in d.groupby(["Groupe", "Pays"]):
        # --- PIB ---
        gdp = sub[sub["Indicateur"].str.lower() == "gdp"].sort_values("Ann√©e")
        risk_gdp = None
        if len(gdp) >= MIN_YEARS_REQUIRED:
            gdp_v = gdp[["Ann√©e", "Valeur"]].dropna()
            if not gdp_v.empty:
                gr = _yoy_growth(gdp_v.set_index("Ann√©e")["Valeur"]).dropna()
                if gr.size >= MIN_YEARS_REQUIRED - 1:
                    gr = _iqr_winsorize(gr)
                    vol = float(gr.std(ddof=1)) if gr.size > 1 else np.nan
                    downside = _semivariance_negative(gr)
                    risk_gdp = vol + 0.2 * downside  # p√©nalisation 20% du downside

        # --- Inflation ---
        infl = sub[sub["Indicateur"].str.lower() == "inflation"].sort_values("Ann√©e")
        risk_infl = None
        if len(infl) >= MIN_YEARS_REQUIRED:
            iv = infl["Valeur"].dropna()
            if iv.size >= MIN_YEARS_REQUIRED:
                iv = _iqr_winsorize(iv)
                vol = float(iv.std(ddof=1)) if iv.size > 1 else np.nan
                # √©cart moyen √† la zone de confort [2,5]
                gap = (np.maximum(0, TARGET_INFL_LOWER - iv) + np.maximum(0, iv - TARGET_INFL_UPPER)).mean()
                risk_infl = vol + 0.7 * float(gap)  # 70% de poids sur l'√©cart de niveau

        # --- Ch√¥mage ---
        unemp = sub[sub["Indicateur"].str.lower().isin(["ch√¥mage", "chomage"])].sort_values("Ann√©e")
        risk_unemp = None
        if len(unemp) >= MIN_YEARS_REQUIRED:
            uv = unemp["Valeur"].dropna()
            if uv.size >= MIN_YEARS_REQUIRED:
                uv = _iqr_winsorize(uv)
                vol = float(uv.std(ddof=1)) if uv.size > 1 else np.nan
                spike = float(uv.quantile(0.95) - uv.median())
                risk_unemp = vol + 0.3 * spike

        n_obs = int(sub.dropna(subset=["Valeur"]).shape[0])
        results.append(MetricResult(ctry, grp, n_obs, risk_gdp, risk_infl, risk_unemp))

    out = pd.DataFrame([r.__dict__ for r in results])
    return out



def compute_country_metrics_per_year(df_long: pd.DataFrame,
                                     min_years: int = MIN_YEARS_REQUIRED,
                                     period: List[int] = PERIOD) -> pd.DataFrame:
    """
    Pour chaque (Groupe, Pays) et chaque ann√©e dans period, calcule les m√©triques:
    - risk_gdp : vol(YoY croissance) + 0.2 * downside (fen√™tre historique jusqu'√† l'ann√©e courante,
      en prenant les derni√®res `min_years` observations si disponibles)
    - risk_infl : vol(inflation) + 0.7 * gap (√©cart moyen √† la zone [2,5])
    - risk_unemp: vol(ch√¥mage) + 0.3 * spike (95e percentile - median)
    Retourne DataFrame country √ó year avec colonnes:
    ['group','country','Ann√©e','n_obs','risk_gdp','risk_infl','risk_unemp']
    """
    df = df_long.copy()
    # homog√©n√©iser nom colonnes
    if "Year" in df.columns and "Ann√©e" not in df.columns:
        df = df.rename(columns={"Year": "Ann√©e"})
    df["Indicateur"] = df["Indicateur"].astype(str)
    results = []

    # d√©tection souple des libell√©s
    def is_gdp(lbl): return "gdp" in lbl.lower() or "pib" in lbl.lower()
    def is_infl(lbl): return "infl" in lbl.lower()
    def is_unemp(lbl): return "chomag" in lbl.lower() or "ch√¥mage" in lbl.lower() or "unemp" in lbl.lower()

    # it√©rer par pays
    grouped = df.groupby(["Groupe", "Pays"])
    for (grp, ctry), sub in grouped:
        # s√©parer s√©ries par indicateur et indexer par ann√©e
        gdp_s = sub[sub["Indicateur"].apply(is_gdp)][["Ann√©e", "Valeur"]].dropna().set_index("Ann√©e").sort_index()["Valeur"]
        infl_s = sub[sub["Indicateur"].apply(is_infl)][["Ann√©e", "Valeur"]].dropna().set_index("Ann√©e").sort_index()["Valeur"]
        unemp_s = sub[sub["Indicateur"].apply(is_unemp)][["Ann√©e", "Valeur"]].dropna().set_index("Ann√©e").sort_index()["Valeur"]

        # ann√©es √† √©valuer : intersection entre period et ann√©es observ√©es ? On gardera period
        for year in period:
            # fenetre: prendre les derni√®res min_years observations **‚â§ year**
            def last_window(series: pd.Series):
                s = series[series.index <= year]
                if s.empty:
                    return s
                return s.tail(min(len(s), min_years))

            # GDP
            risk_gdp = None
            gdp_w = last_window(gdp_s)
            if len(gdp_w) >= min_years:
                gr =  _yoy_growth(gdp_w).dropna()
                if gr.size >= max(1, min_years-1):
                    gr = _iqr_winsorize(gr)
                    vol = float(gr.std(ddof=1)) if gr.size > 1 else 0.0
                    downside = _semivariance_negative(gr)
                    risk_gdp = vol + 0.2 * downside

            # Inflation
            risk_infl = None
            infl_w = last_window(infl_s)
            if len(infl_w) >= min_years:
                iv = infl_w.copy()
                iv = _iqr_winsorize(iv)
                vol = float(iv.std(ddof=1)) if iv.size > 1 else 0.0
                # gap moyen √† la zone [TARGET_INFL_LOWER, TARGET_INFL_UPPER]
                gap = (np.maximum(0, TARGET_INFL_LOWER - iv) + np.maximum(0, iv - TARGET_INFL_UPPER)).mean()
                risk_infl = vol + 0.7 * float(gap)

            # Ch√¥mage
            risk_unemp = None
            unemp_w = last_window(unemp_s)
            if len(unemp_w) >= min_years:
                uv = unemp_w.copy()
                uv = _iqr_winsorize(uv)
                vol = float(uv.std(ddof=1)) if uv.size > 1 else 0.0
                spike = float(uv.quantile(0.95) - uv.median())
                risk_unemp = vol + 0.3 * spike

            # nombre d'observations utiles dans la fen√™tre (somme des non-nulls)
            n_obs = int(gdp_w.count() + infl_w.count() + unemp_w.count())

            results.append({
                "group": grp,
                "country": ctry,
                "Ann√©e": int(year),
                "n_obs": n_obs,
                "risk_gdp": risk_gdp,
                "risk_infl": risk_infl,
                "risk_unemp": risk_unemp
            })

    out = pd.DataFrame(results)
    # trier
    out = out.sort_values(["group", "country", "Ann√©e"]).reset_index(drop=True)
    return out


# ============================
# üìä Fonctions utilitaires
# ============================

import pandas as pd
import numpy as np

def categorize_scores(x: pd.Series,
                      q_low: float = 0.5,
                      q_stress: float = 0.9,
                      ref_values: pd.Series | None = None) -> pd.Series:
    """
    Classe les scores en cat√©gories 'Faible', 'Moyen', '√âlev√©' selon la logique BCE/Fed.
    
    - Calibrage sur des quantiles fixes (m√©diane, 90e percentile)
    - Si ref_values est fourni, les seuils sont calibr√©s sur cette p√©riode de r√©f√©rence
      (garantissant la constance temporelle du stress)
    
    Param√®tres
    ----------
    x : pd.Series
        S√©rie de scores (risk ou score_composite)
    q_low : float
        Quantile inf√©rieur (par d√©faut 0.5 = m√©diane)
    q_stress : float
        Quantile sup√©rieur d√©finissant la zone de stress (par d√©faut 0.9)
    ref_values : pd.Series | None
        Valeurs historiques utilis√©es pour calibrer les seuils.
        Si None, utilise x lui-m√™me.
    
    Retour
    ------
    pd.Series
        S√©rie de classes : 'Faible', 'Moyen', '√âlev√©'
    """
    # Choisir base de calibration
    calib = ref_values.dropna() if ref_values is not None else x.dropna()

    if calib.nunique() <= 1:
        return pd.Series(["Moyen"] * len(x), index=x.index)

    # Seuils calibr√©s sur la p√©riode de r√©f√©rence
    ql = calib.quantile(q_low)
    qs = calib.quantile(q_stress)

    # Application des r√®gles BCE/Fed
    classes = pd.cut(
        x,
        bins=[-np.inf, ql, qs, np.inf],
        labels=["Faible", "Moyen", "√âlev√©"],
        include_lowest=True
    ).astype(str)

    return classes





# ============================
# üß™ Normalisation & Score (par ann√©e)
# ============================

def build_scores_per_year(metrics_year: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    Construit les scores normalis√©s et classes de risque par pays et par groupe, pour chaque ann√©e.
    """

    rows_pays = []
    rows_groupes = []

    for year, df_y in metrics_year.groupby("Ann√©e"):
        df = df_y.copy()

        # Normalisation 0..100
        for col, normcol in [("risk_gdp", "score_gdp"),
                             ("risk_infl", "score_infl"),
                             ("risk_unemp", "score_unemp")]:
            df[normcol] = _min_max_norm(df[col]) * 100.0

        # Score composite
        df["score_composite"] = (
            WEIGHTS["gdp"] * df["score_gdp"].fillna(df["score_gdp"].median()) +
            WEIGHTS["inflation"] * df["score_infl"].fillna(df["score_infl"].median()) +
            WEIGHTS["unemp"] * df["score_unemp"].fillna(df["score_unemp"].median())
        )

        # Rang pays
        df["rang_pays"] = df["score_composite"].rank(method="min", ascending=False).astype(int)

        # Cat√©gorisation robuste
        df["classe_risque"] = categorize_scores(df["score_composite"])

        # Renommage
        df = df.rename(columns={
            "country": "Pays",
            "group": "Groupe",
            "score_gdp": "Score PIB (vol. croissance)",
            "score_infl": "Score Inflation (niv.+vol.)",
            "score_unemp": "Score Ch√¥mage (vol.)",
            "score_composite": "Score Composite (0-100)",
        })

        df["Ann√©e"] = year
        rows_pays.append(df)

        # Agr√©gation groupe (m√©diane)
        grp = (
            df.groupby("Groupe")
              .agg(**{
                  "Score PIB (vol. croissance)": ("Score PIB (vol. croissance)", "median"),
                  "Score Inflation (niv.+vol.)": ("Score Inflation (niv.+vol.)", "median"),
                  "Score Ch√¥mage (vol.)": ("Score Ch√¥mage (vol.)", "median"),
                  "Score Composite (0-100)": ("Score Composite (0-100)", "median"),
                  "nb_pays": ("Pays", "nunique"),
              })
              .reset_index()
        )

        grp["rang_groupe"] = grp["Score Composite (0-100)"].rank(method="min", ascending=False).astype(int)
        grp["classe_risque"] = categorize_scores(grp["Score Composite (0-100)"])
        grp["Ann√©e"] = year

        rows_groupes.append(grp)

    df_pays_year = pd.concat(rows_pays, axis=0).reset_index(drop=True)
    df_groupes_year = pd.concat(rows_groupes, axis=0).reset_index(drop=True)

    return df_pays_year, df_groupes_year



# ============================
# üßµ Pipeline complet
# ============================


def compute_macro_risk_pipeline(df_input: pd.DataFrame,
                                export_excel: Optional[str] = None) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    Pipeline de calcul des scores macro√©conomiques.

    Param√®tres
    ----------
    df_input : pd.DataFrame
        DataFrame au format large ou long (voir doc en t√™te).
    export_excel : str, optionnel
        Chemin complet du fichier .xlsx pour exporter les r√©sultats.
        Exemple : "05-Result/scores/macro_scores.xlsx"

    Retour
    ------
    (df_pays, df_groupes)
    """
    # üîπ Harmonisation du format
    df_long = ensure_long_format(df_input)

    # üîπ Nettoyage basique
    df_long = df_long.dropna(subset=["Groupe", "Pays", "Indicateur"]).copy()
    df_long["Indicateur"] = df_long["Indicateur"].astype(str)

    # Harmoniser les libell√©s
    df_long["Indicateur"] = df_long["Indicateur"].str.replace("chomage", "Ch√¥mage", case=False)
    df_long["Indicateur"] = df_long["Indicateur"].str.replace("ch√¥mage", "Ch√¥mage", case=False)

    # üîπ Calcul des m√©triques
    metrics = compute_country_metrics(df_long)

    # üîπ Construction des scores
    df_pays, df_groupes = build_scores(metrics)

    # üîπ Export si demand√©
    if export_excel:
        folder = os.path.dirname(export_excel)
        if folder:  # si un dossier est indiqu√©
            os.makedirs(folder, exist_ok=True)

        with pd.ExcelWriter(export_excel, engine="xlsxwriter") as xw:
            df_pays.to_excel(xw, index=False, sheet_name="Scores_Pays")
            df_groupes.to_excel(xw, index=False, sheet_name="Scores_Groupes")

        print(f"‚úÖ R√©sultats export√©s vers : {export_excel}")

    return df_pays, df_groupes



def compute_macro_risk_pipeline_per_year(df_input: pd.DataFrame,
                                         export_excel: Optional[str] = None) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    Pipeline complet de calcul macro par ann√©e
    """
    df_long = ensure_long_format(df_input)

    # Nettoyage basique
    df_long = df_long.dropna(subset=["Groupe", "Pays", "Indicateur"]).copy()
    df_long["Indicateur"] = df_long["Indicateur"].astype(str)
    df_long["Indicateur"] = df_long["Indicateur"].str.replace("chomage", "Ch√¥mage", case=False)
    df_long["Indicateur"] = df_long["Indicateur"].str.replace("ch√¥mage", "Ch√¥mage", case=False)

    # Calcul des m√©triques par ann√©e
    metrics_year = compute_country_metrics_per_year(df_long,
                                                    min_years=MIN_YEARS_REQUIRED,
                                                    period=PERIOD) # compute_country_metrics_per_year  compute_country_metrics

    # Construction des scores
    df_pays_year, df_groupes_year = build_scores_per_year(metrics_year)

    # Export Excel si demand√©
    if export_excel:
        folder = os.path.dirname(export_excel)
        if folder:
            os.makedirs(folder, exist_ok=True)

        with pd.ExcelWriter(export_excel, engine="xlsxwriter") as xw:
            df_pays_year.to_excel(xw, index=False, sheet_name="Scores_Pays_Ann√©e")
            df_groupes_year.to_excel(xw, index=False, sheet_name="Scores_Groupes_Ann√©e")

        print(f"‚úÖ R√©sultats export√©s vers : {export_excel}")

    return df_pays_year, df_groupes_year


